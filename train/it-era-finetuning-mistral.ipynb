{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8029471,"sourceType":"datasetVersion","datasetId":4496808},{"sourceId":8029496,"sourceType":"datasetVersion","datasetId":4732600},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\n\n# specify your directory\ndir_path = '/kaggle/working/'\n\n# remove all files in the directory\nfor filename in os.listdir(dir_path):\n    file_path = os.path.join(dir_path, filename)\n    try:\n        if os.path.isfile(file_path) or os.path.islink(file_path):\n            os.unlink(file_path)\n        elif os.path.isdir(file_path):\n            shutil.rmtree(file_path)\n    except Exception as e:\n        print(f'Failed to delete {file_path}. Reason: {e}')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:22:58.382424Z","iopub.execute_input":"2024-04-07T13:22:58.382815Z","iopub.status.idle":"2024-04-07T13:22:59.432831Z","shell.execute_reply.started":"2024-04-07T13:22:58.382782Z","shell.execute_reply":"2024-04-07T13:22:59.432023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl","metadata":{"id":"fNZZilUV_L4r","outputId":"31a2fa06-284f-411a-f439-0e8f949bfaf8","execution":{"iopub.status.busy":"2024-04-07T13:23:07.844127Z","iopub.execute_input":"2024-04-07T13:23:07.844777Z","iopub.status.idle":"2024-04-07T13:24:30.464000Z","shell.execute_reply.started":"2024-04-07T13:23:07.844746Z","shell.execute_reply":"2024-04-07T13:24:30.462806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n%pip install -U datasets","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:24:30.466129Z","iopub.execute_input":"2024-04-07T13:24:30.466487Z","iopub.status.idle":"2024-04-07T13:24:59.928739Z","shell.execute_reply.started":"2024-04-07T13:24:30.466458Z","shell.execute_reply":"2024-04-07T13:24:59.927407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n%pip install -U wandb\n%pip install -U matplotlib scipy\n%pip install -U evaluate","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:24:59.930261Z","iopub.execute_input":"2024-04-07T13:24:59.930575Z","iopub.status.idle":"2024-04-07T13:26:00.430435Z","shell.execute_reply.started":"2024-04-07T13:24:59.930549Z","shell.execute_reply":"2024-04-07T13:26:00.429368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%pip install --user --force-reinstall --no-deps numpy==1.23","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging, DataCollatorForLanguageModeling\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\n\nimport numpy as np\nimport evaluate","metadata":{"id":"VLzgZ14X_rMs","execution":{"iopub.status.busy":"2024-04-07T13:26:00.433513Z","iopub.execute_input":"2024-04-07T13:26:00.434348Z","iopub.status.idle":"2024-04-07T13:26:21.072827Z","shell.execute_reply.started":"2024-04-07T13:26:00.434311Z","shell.execute_reply":"2024-04-07T13:26:21.071816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_wandb = user_secrets.get_secret(\"wandb\")","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:26:21.074145Z","iopub.execute_input":"2024-04-07T13:26:21.074924Z","iopub.status.idle":"2024-04-07T13:26:21.360353Z","shell.execute_reply.started":"2024-04-07T13:26:21.074891Z","shell.execute_reply":"2024-04-07T13:26:21.359450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"id":"f9oI6bPHzpDW","outputId":"07b4bb79-e35e-43a9-c0a9-fabbb4ac2721","execution":{"iopub.status.busy":"2024-04-07T13:26:21.361408Z","iopub.execute_input":"2024-04-07T13:26:21.361696Z","iopub.status.idle":"2024-04-07T13:26:23.046534Z","shell.execute_reply.started":"2024-04-07T13:26:21.361659Z","shell.execute_reply":"2024-04-07T13:26:23.045498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Monitering the LLM\nwandb.login(key = secret_wandb)\nrun = wandb.init(\n    project='Fine tuning mistral 7B', \n    job_type=\"training\", \n    anonymous=\"allow\",\n    name=\"IT-Era-Run-v2\",\n    resume=\"allow\",\n)","metadata":{"id":"na9CAoHC5gM9","execution":{"iopub.status.busy":"2024-04-07T13:26:23.047921Z","iopub.execute_input":"2024-04-07T13:26:23.048225Z","iopub.status.idle":"2024-04-07T13:26:42.438394Z","shell.execute_reply.started":"2024-04-07T13:26:23.048199Z","shell.execute_reply":"2024-04-07T13:26:42.437326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#base_model = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n#replace the base when >3\n#base_model= \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\n#dataset_name = \"/kaggle/input/essay-train-v2\"\n#new_model = \"mistral_7b_AES_v2_max-steps\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model=\"mistralai/Mistral-7B-Instruct-v0.2\" \ntrain_dataset_name=\"/kaggle/input/essay-train-v3\"\ntest_dataset_name=\"/kaggle/input/essay-validate\"\nnew_model = \"mistral_7b_AES_v2\"","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:26:42.439643Z","iopub.execute_input":"2024-04-07T13:26:42.440923Z","iopub.status.idle":"2024-04-07T13:26:42.449043Z","shell.execute_reply.started":"2024-04-07T13:26:42.440897Z","shell.execute_reply":"2024-04-07T13:26:42.447739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing the dataset\ntrain_data = load_dataset(train_dataset_name, split=\"train\")\nvalidate_data =load_dataset(test_dataset_name, split=\"train\")","metadata":{"id":"XzF2UjPvTBag","outputId":"3733e45f-605e-4564-88c7-368c9c5bf9cd","execution":{"iopub.status.busy":"2024-04-07T13:26:42.450398Z","iopub.execute_input":"2024-04-07T13:26:42.450706Z","iopub.status.idle":"2024-04-07T13:26:43.401346Z","shell.execute_reply.started":"2024-04-07T13:26:42.450656Z","shell.execute_reply":"2024-04-07T13:26:43.400220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validate_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load base model(Mistral 7B)\nbnb_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\nmodel.config.use_cache = False # silence the warnings. Please re-enable for inference!\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token\n\n\n#tokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"id":"StJKGiDDHzdk","outputId":"871214ba-6c30-4ecf-ac68-550f296b7ef6","execution":{"iopub.status.busy":"2024-04-07T13:26:43.405281Z","iopub.execute_input":"2024-04-07T13:26:43.406169Z","iopub.status.idle":"2024-04-07T13:29:16.691080Z","shell.execute_reply.started":"2024-04-07T13:26:43.406135Z","shell.execute_reply":"2024-04-07T13:29:16.690084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_prompt(data_point):\n    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n\n    :param data_point: dict: Data point\n    :return: dict: tokenized prompt\n    \"\"\"\n    #prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\\n    #           'appropriately completes the request.\\n\\n'\n    # Samples with additional context into.\n    if data_point['input']:\n        text = f\"\"\"<s>[INST]{data_point[\"instruction\"]}\\n{data_point[\"input\"]} [/INST]{data_point[\"output\"]}</s>\"\"\"\n    # Without\n    else:\n        text = f\"\"\"<s>[INST]{data_point[\"instruction\"]} [/INST]{data_point[\"output\"]} </s>\"\"\"\n    return text\n\n#add the \"prompt\" column in the dataset\ntrain_text_column = [generate_prompt(data_point) for data_point in train_data]\ntest_text_column = [generate_prompt(data_point) for data_point in validate_data]\n\ntrain_data = train_data.add_column(\"text\", train_text_column)\nvalidate_data = validate_data.add_column(\"text\", test_text_column)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:29:16.692357Z","iopub.execute_input":"2024-04-07T13:29:16.693776Z","iopub.status.idle":"2024-04-07T13:29:16.812726Z","shell.execute_reply.started":"2024-04-07T13:29:16.693730Z","shell.execute_reply":"2024-04-07T13:29:16.811741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming `dataset` is your data and 'prompt' is the key where sequences are stored\nsequences = train_data['text']\nlengths = [len(x['text']) for x in train_data]\n\n#print(lengths)\n\n# Plotting the histogram\nplt.figure(figsize=(10, 6))\nplt.hist(lengths, bins=20, alpha=0.7)\nplt.xlabel('Length of input_ids')\nplt.ylabel('Frequency')\nplt.title('Distribution of Lengths of input_ids')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:59:21.754989Z","iopub.execute_input":"2024-04-07T05:59:21.755368Z","iopub.status.idle":"2024-04-07T05:59:22.188869Z","shell.execute_reply.started":"2024-04-07T05:59:21.755341Z","shell.execute_reply":"2024-04-07T05:59:22.187198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_length = 3300\ncol_to_delete = ['input', 'instruction', 'output', 'filename']\n\ntrain_data = train_data.map(lambda samples: tokenizer(samples[\"text\"], truncation=True, max_length=max_seq_length, padding=\"max_length\"), batched=True, remove_columns=col_to_delete)\nvalidate_data = validate_data.map(lambda samples: tokenizer(samples[\"text\"], truncation=True, max_length=max_seq_length, padding=\"max_length\"), batched=True, remove_columns=col_to_delete)\n\n\ntrain_data = train_data.add_column(\"labels\", train_data['input_ids'])\nvalidate_data = validate_data.add_column(\"labels\", validate_data ['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:29:16.814040Z","iopub.execute_input":"2024-04-07T13:29:16.814374Z","iopub.status.idle":"2024-04-07T13:29:28.765275Z","shell.execute_reply.started":"2024-04-07T13:29:16.814343Z","shell.execute_reply":"2024-04-07T13:29:28.764107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.set_format(\"torch\")\nvalidate_data.set_format(\"torch\")","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:29:28.766581Z","iopub.execute_input":"2024-04-07T13:29:28.766965Z","iopub.status.idle":"2024-04-07T13:29:28.774855Z","shell.execute_reply.started":"2024-04-07T13:29:28.766939Z","shell.execute_reply":"2024-04-07T13:29:28.773746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming `dataset` is your data and 'prompt' is the key where sequences are stored\nsequences = train_data['input_ids']\nlengths = [len(x['input_ids']) for x in train_data]\n\n#print(lengths)\n\n# Plotting the histogram\nplt.figure(figsize=(10, 6))\nplt.hist(lengths, bins=20, alpha=0.7)\nplt.xlabel('Length of input_ids')\nplt.ylabel('Frequency')\nplt.title('Distribution of Lengths of input_ids')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:59:37.259734Z","iopub.execute_input":"2024-04-07T05:59:37.260153Z","iopub.status.idle":"2024-04-07T05:59:37.829469Z","shell.execute_reply.started":"2024-04-07T05:59:37.260115Z","shell.execute_reply":"2024-04-07T05:59:37.827857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2024-04-07T05:59:43.606368Z","iopub.execute_input":"2024-04-07T05:59:43.606711Z","iopub.status.idle":"2024-04-07T05:59:43.614431Z","shell.execute_reply.started":"2024-04-07T05:59:43.606686Z","shell.execute_reply":"2024-04-07T05:59:43.613411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validate_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save dataset para ma resume mao nalng ni gamiton sunod run \n#dataset.save_to_disk('/kaggle/working/my_dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.device_count() > 1: # If more than 1 GPU\n    print(torch.cuda.device_count())\n    model.is_parallelizable = True\n    model.model_parallel = True","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:29:28.776253Z","iopub.execute_input":"2024-04-07T13:29:28.776605Z","iopub.status.idle":"2024-04-07T13:29:28.786962Z","shell.execute_reply.started":"2024-04-07T13:29:28.776568Z","shell.execute_reply":"2024-04-07T13:29:28.785703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adding the adapters in the layers\n\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)","metadata":{"id":"DZoClwHcPTqQ","execution":{"iopub.status.busy":"2024-04-07T13:29:28.788324Z","iopub.execute_input":"2024-04-07T13:29:28.788730Z","iopub.status.idle":"2024-04-07T13:29:30.189607Z","shell.execute_reply.started":"2024-04-07T13:29:28.788700Z","shell.execute_reply":"2024-04-07T13:29:30.188544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_arguments=TrainingArguments(\n    output_dir = \"Mistral_AES_v2\",\n    warmup_steps=1,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=1,\n    gradient_checkpointing=True,\n    #max_steps=500,\n    num_train_epochs=7, # changed to 3 \n    weight_decay=0.001,\n    learning_rate=2.5e-5,                # Want a small lr for finetuning\n    fp16=False,                          #might need to set this to true\n    bf16=False,\n    optim=\"paged_adamw_32bit\",\n    logging_steps=50,                    # When to start reporting loss\n    logging_dir=\"/kaggle/working/logs\",  # Directory for storing logs\n    save_strategy=\"epoch\",               # Save the model checkpoint every step\n    #save_steps=287, #287                # Save checkpoints every 96 steps 1/3 each epoch\n    evaluation_strategy=\"epoch\",         # Evaluate the model every logging step\n    eval_steps=50,                       # Evaluate and save checkpoints every 287 steps\n    do_eval=True,                        # Perform evaluation at the end of training\n    report_to=\"wandb\",                   # Comment this out if you don't want to use weights & baises        # Name of the W&B run (optional)\n    run_name=\"IT_Era_Run_Epoch\",                # Name of the W&B run (optional)\n    lr_scheduler_type=\"constant\",\n    load_best_model_at_end=True,\n    save_total_limit=8,\n    do_predict=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:29:30.190889Z","iopub.execute_input":"2024-04-07T13:29:30.191174Z","iopub.status.idle":"2024-04-07T13:29:30.239575Z","shell.execute_reply.started":"2024-04-07T13:29:30.191150Z","shell.execute_reply":"2024-04-07T13:29:30.238537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#metric = evaluate.load(\"accuracy\")\n\n#def compute_metrics(eval_pred):\n#    logits, labels = eval_pred\n#    predictions = np.argmax(logits, axis=-1)\n#    return metric.compute(predictions=predictions, references=labels)\n\ndef compute_metrics(eval_pred):\n    # All metrics are already predefined in the HF `evaluate` package\n    precision_metric = evaluate.load(\"precision\")\n    #recall_metric = evaluate.load(\"recall\")\n    f1_metric= evaluate.load(\"f1\")\n    accuracy_metric = evaluate.load(\"accuracy\")\n\n    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n    predictions = np.argmax(logits, axis=-1)\n    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n    #recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    # The trainer is expecting a dictionary where the keys are the metrics names and the values are the scores. \n    return {\"precision\": precision, \"f1-score\": f1, 'accuracy': accuracy}\n\ndef preprocess_logits_for_metrics(logits, labels):\n    \"\"\"\n    Original Trainer may have a memory leak. \n    This is a workaround to avoid storing too many tensors that are not needed.\n    \"\"\"\n    pred_ids = torch.argmax(logits[0], dim=-1)\n    return pred_ids, labels","metadata":{"execution":{"iopub.status.busy":"2024-04-06T15:19:44.080800Z","iopub.execute_input":"2024-04-06T15:19:44.081348Z","iopub.status.idle":"2024-04-06T15:19:44.090862Z","shell.execute_reply.started":"2024-04-06T15:19:44.081315Z","shell.execute_reply":"2024-04-06T15:19:44.089836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\nmax_seq_length = 3300\n\ntrainer = SFTTrainer(\n  model=model,\n  peft_config=peft_config,\n  max_seq_length=max_seq_length,\n  tokenizer=tokenizer,\n  #packing=False,\n  args=training_arguments,\n  dataset_text_field=\"text\",\n  train_dataset=train_data,\n  eval_dataset=validate_data,\n  data_collator=collator,\n  #compute_metrics=compute_metrics,\n  #preprocess_logits_for_metrics=preprocess_logits_for_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:29:30.240950Z","iopub.execute_input":"2024-04-07T13:29:30.241334Z","iopub.status.idle":"2024-04-07T13:29:30.765079Z","shell.execute_reply.started":"2024-04-07T13:29:30.241296Z","shell.execute_reply":"2024-04-07T13:29:30.764095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trainer.train(resume_from_checkpoint=\"/kaggle/working/Mistral_AES_v2/checkpoint-574\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train() ","metadata":{"execution":{"iopub.status.busy":"2024-04-07T13:29:30.766436Z","iopub.execute_input":"2024-04-07T13:29:30.767140Z","iopub.status.idle":"2024-04-07T23:57:26.236994Z","shell.execute_reply.started":"2024-04-07T13:29:30.767107Z","shell.execute_reply":"2024-04-07T23:57:26.235731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T23:57:26.238383Z","iopub.execute_input":"2024-04-07T23:57:26.238759Z","iopub.status.idle":"2024-04-07T23:57:30.643080Z","shell.execute_reply.started":"2024-04-07T23:57:26.238724Z","shell.execute_reply":"2024-04-07T23:57:30.642111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True\nmodel.eval()","metadata":{"id":"nKgZBEGVS5a2","execution":{"iopub.status.busy":"2024-04-07T23:57:30.644306Z","iopub.execute_input":"2024-04-07T23:57:30.644620Z","iopub.status.idle":"2024-04-07T23:57:31.382818Z","shell.execute_reply.started":"2024-04-07T23:57:30.644592Z","shell.execute_reply":"2024-04-07T23:57:31.382051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model_checkpoint = trainer.state.best_model_checkpoint\nbest_model_checkpoint ","metadata":{"execution":{"iopub.status.busy":"2024-04-07T23:57:31.383850Z","iopub.execute_input":"2024-04-07T23:57:31.384106Z","iopub.status.idle":"2024-04-07T23:57:31.390004Z","shell.execute_reply.started":"2024-04-07T23:57:31.384084Z","shell.execute_reply":"2024-04-07T23:57:31.389017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#try:\n#    trainer.model.push_to_hub(new_model, use_temp_dir=False)\n#except:\n#    print(\"An exception occurred\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!1eval_results = trainer.evaluate()\n\n# Print the accuracy\n#print(eval_results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}